{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2dfbaf53-32de-4b8c-bd1c-d27371a87f81",
      "metadata": {
        "id": "2dfbaf53-32de-4b8c-bd1c-d27371a87f81"
      },
      "source": [
        "# Using file search tool in the Responses API\n",
        "\n",
        "Although RAG can be overwhelming, searching amongst PDF file shouldn't be complicated. One of the most adopted options as of now is parsing your PDF, defining your chunking strategies, uploading those chunks to a storage provider, running embeddings on those chunks of texts and storing those embeddings in a vector database. And that's only the setup — retrieving content in our LLM workflow also requires multiple steps.\n",
        "\n",
        "This is where file search — a hosted tool you can use in the Responses API — comes in. It allows you to search your knowledge base and generate an answer based on the retrieved content. In this cookbook, we'll upload those PDFs to a vector store on OpenAI and use file search to fetch additional context from this vector store to answer the questions we generated in the first step. Then, we'll initially create a small set of questions based on PDFs extracted from OpenAI's blog ([openai.com/news](https://openai.com/news)).\n",
        "\n",
        "_File search was previously available on the Assistants API. It's now available on the new Responses API, an API that can be stateful or stateless, and with from new features like metadata filtering_\n",
        "\n",
        "### Set up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "47480955-9dd4-4837-8b4c-6821bb48306b",
      "metadata": {
        "id": "47480955-9dd4-4837-8b4c-6821bb48306b",
        "outputId": "2ca6a15a-f0a2-40b1-fab3-e18ac23177d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m872.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2 pandas tqdm sentence-transformers faiss-cpu transformers torch -q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install python-dotenv"
      ],
      "metadata": {
        "id": "TUCmp5--JLWk",
        "outputId": "6a406a8c-45e1-4597-f7fb-33b3e15955bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TUCmp5--JLWk",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import PyPDF2\n",
        "import pandas as pd\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OBz3brG2QidP"
      },
      "id": "OBz3brG2QidP",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "dir_pdfs = 'openai_blog_pdfs' # have those PDFs stored locally here\n",
        "if not os.path.exists(dir_pdfs):\n",
        "  print(f\"Directory '{dir_pdfs}' not found. Please create it and place your PDF files inside.\")\n",
        "\n",
        "else:\n",
        "  pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs) if f.endswith('.pdf')]\n",
        "if not pdf_files:\n",
        "  print(f\"No PDF files found in '{dir_pdfs}'. Please add some PDF files to the directory.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "09_n2bzARE7-"
      },
      "id": "09_n2bzARE7-",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "   \"\"\"Extracts text from a single PDF file.\"\"\"\n",
        "   text = \"\"\n",
        "   try:\n",
        "    with open(pdf_path, \"rb\") as f:\n",
        "      reader = PyPDF2.PdfReader(f)\n",
        "      for page in reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "\n",
        "                # Simple chunking strategy: treat each page as a chunk\n",
        "                # You might need a more sophisticated chunking method for better results\n",
        "                text += page_text + \"\\n----PAGE_BREAK----\\n\" # Add a marker to potentially split later\n",
        "   except Exception as e:\n",
        "    print(f\"Error reading {pdf_path}: {e}\")\n",
        "   return text"
      ],
      "metadata": {
        "id": "t3SKIS6dRi8X"
      },
      "id": "t3SKIS6dRi8X",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text, chunk_size=500, chunk_overlap=50):\n",
        "  \"\"\"Splits text into smaller chunks with overlap.\"\"\"\n",
        "  chunks = []\n",
        "\n",
        "  # A more robust chunking strategy would be beneficial here\n",
        "  # For simplicity, we'll split by page break and then by a fixed size if pages are too large\n",
        "  pages = text.split(\"\\n----PAGE_BREAK----\\n\")\n",
        "  for page in pages:\n",
        "    if len(page) > chunk_size:\n",
        "        words = page.split()\n",
        "        for i in range(0, len(words), chunk_size - chunk_overlap):\n",
        "            chunk = \" \".join(words[i:i + chunk_size])\n",
        "            if chunk:\n",
        "                chunks.append(chunk)\n",
        "    elif page.strip():\n",
        "        chunks.append(page.strip())\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "HJHRnd83TA1F"
      },
      "id": "HJHRnd83TA1F",
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_embeddings(chunks, model):\n",
        "   \"\"\"Generates embeddings for text chunks using a SentenceTransformer model.\"\"\"\n",
        "   print(f\"Creating embeddings for {len(chunks)} chunks...\")\n",
        "   embeddings = model.encode(chunks, show_progress_bar=True)\n",
        "   return embeddings"
      ],
      "metadata": {
        "id": "sPMkA-ULTj-l"
      },
      "id": "sPMkA-ULTj-l",
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "embedding_dimension = embedding_model.get_sentence_embedding_dimension()\n",
        "\n",
        "all_chunks = []\n",
        "chunk_metadata = [] # To store info like source file for each chunk\n",
        "\n",
        "print(f\"Processing {len(pdf_files)} PDF files...\")\n",
        "for pdf_file in tqdm(pdf_files):\n",
        "  text = extract_text_from_pdf(pdf_file)\n",
        "  chunks = chunk_text(text)\n",
        "  all_chunks.extend(chunks)\n",
        "  for i, chunk in enumerate(chunks):\n",
        "    chunk_metadata.append({\"source\": os.path.basename(pdf_file), \"chunk_index\": i})"
      ],
      "metadata": {
        "id": "C8NyeVeDT9RP",
        "outputId": "cd246385-0adc-4cab-e822-682ccfb72789",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "C8NyeVeDT9RP",
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing 1 PDF files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chunk_embeddings = create_embeddings(all_chunks, embedding_model)\n",
        "index = faiss.IndexFlatL2(embedding_dimension)\n",
        "index.add(np.array(chunk_embeddings))\n",
        "\n",
        "print(f\"FAISS index created with {index.ntotal} embeddings.\")\n"
      ],
      "metadata": {
        "id": "b6GQKl31V38a",
        "outputId": "92d05194-c21f-4673-b671-5fb88f2c23a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "6eee51142df3499aa7705a686a5b7b1c",
            "8907ae51048142fcbe8fb9cc86494ccd",
            "61f365f727164f0e805574e60bb5082e",
            "1c3d5ecb909d4d58b74ea65573e8c035",
            "7d459cfe0ebd48bca6f4f29a6f4ae7d9",
            "1e5f384dcd0d487a89db977f006e8c93",
            "2edc3327b9164abf8a5478a70e6138eb",
            "dfa5ad9c84834fb2ba67781d99e97ce0",
            "b2c5d568ca544dc6855b00e52524d56a",
            "f6f056ba4c0b4e3ebb641d33e61fdabf",
            "697b12d398ef45fdb44ff9cc532b36bd"
          ]
        }
      },
      "id": "b6GQKl31V38a",
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating embeddings for 92 chunks...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6eee51142df3499aa7705a686a5b7b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index created with 92 embeddings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_chunks(query: str, index, embedding_model, all_chunks, chunk_metadata, k=5):\n",
        " \"\"\"Retrieves top-k most similar chunks for a given query.\"\"\"\n",
        " # Create embedding for the query\n",
        " query_embedding = embedding_model.encode(query)\n",
        " query_embedding = np.array([query_embedding]) # Reshape for FAISS search\n",
        "\n",
        " # Search the FAISS index\n",
        " distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "# Get the retrieved chunks and their metadata\n",
        " retrieved = []\n",
        " for i in indices[0]:\n",
        "    if i != -1: # Check if a valid index was returned\n",
        "         retrieved.append({\n",
        "             \"text\": all_chunks[i],\n",
        "             \"metadata\": chunk_metadata[i],\n",
        "             \"distance\": distances[0][list(indices[0]).index(i)]\n",
        "         })\n",
        " return retrieved\n",
        "\n",
        "try:\n",
        "  generator = pipeline('text-generation', model='distilgpt2')\n",
        "  print(\"Text generation pipeline loaded using distilgpt2.\")\n",
        "except Exception as e:\n",
        "  print(f\"Error loading text generation pipeline: {e}\")\n",
        "  print(\"You might need to choose a smaller model or run this on a machine with more resources.\")\n",
        "  generator = None # Set to None if loading fails\n"
      ],
      "metadata": {
        "id": "IZsHyk5CWX_B",
        "outputId": "62c3ddb5-69d7-423e-d5a5-762d201f8dff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "IZsHyk5CWX_B",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text generation pipeline loaded using distilgpt2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer_with_rag(query: str, retrieved_chunks, generator):\n",
        "   \"\"\"Generates an answer using a free LLM based on retrieved chunks.\"\"\"\n",
        "   if not generator:\n",
        "    return \"Language model not loaded. Cannot generate answer.\"\n",
        "\n",
        "   context = \"\\n\".join([chunk['text'] for chunk in retrieved_chunks])\n",
        "\n",
        "   if not context.strip():\n",
        "      return \"No relevant information found in the documents.\"\n",
        "\n",
        "    # Craft the prompt for the language model\n",
        "    # This prompt structure is crucial for getting good results\n",
        "   prompt = f\"\"\"Based on the following information, answer the query:\n",
        "    Information: {context}\n",
        "\n",
        "   Query: {query}\n",
        "\n",
        "   Answer: \"\"\"\n",
        "\n",
        "   print(\"Generating answer with LLM...\")\n",
        "    # Generate text. You might need to adjust max_new_tokens and other parameters\n",
        "   response = generator(prompt, max_new_tokens=200, num_return_sequences=1, do_sample=True)[0]['generated_text']\n",
        "\n",
        "    # Post-process the response to potentially remove the prompt part\n",
        "    # This is a simple approach, more sophisticated parsing might be needed\n",
        "   answer_prefix = \"Answer:\\n\"\n",
        "   if answer_prefix in response:\n",
        "        answer = response.split(answer_prefix, 1)[1].strip()\n",
        "   else:\n",
        "        answer = response.strip()\n",
        "\n",
        "   return answer"
      ],
      "metadata": {
        "id": "FPNJLyl4XQ1X"
      },
      "id": "FPNJLyl4XQ1X",
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What are the different types of agent\"\n",
        "print(f\"\\nRetrieving chunks for query: '{query}'...\")\n",
        "retrieved_chunks = retrieve_chunks(query, index, embedding_model, all_chunks, chunk_metadata, k=3) # Retrieve top 3 chunks\n",
        "\n",
        "if retrieved_chunks:\n",
        "  print(f\"Found {len(retrieved_chunks)} relevant chunks.\")\n",
        "\n",
        "# Optional: Print retrieved chunks and their source\n",
        "# for i, chunk in enumerate(retrieved_chunks):\n",
        "#     print(f\"--- Chunk {i+1} (Source: {chunk['metadata']['source']}, Distance: {chunk['distance']:.4f}) ---\")\n",
        "#     print(chunk['text'][:300] + \"...\") # Print first 300 characters\n",
        "#     print(\"-\" * 20)\n",
        "\n",
        "# 2. Generate answer using the LLM and retrieved chunks\n",
        "  answer = generate_answer_with_rag(query, retrieved_chunks, generator)\n",
        "\n",
        "  print(\"\\nGenerated Answer:\")\n",
        "  print(answer)\n",
        "else: print(\"No relevant chunks found for the query.\")"
      ],
      "metadata": {
        "id": "ML3E7LrhXQx7",
        "outputId": "6e822121-0198-4178-cd8f-0b5d16dbc85a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ML3E7LrhXQx7",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retrieving chunks for query: 'What are the different types of agent'...\n",
            "Found 3 relevant chunks.\n",
            "Generating answer with LLM...\n",
            "\n",
            "Generated Answer:\n",
            "Based on the following information, answer the query:\n",
            "    Information: Contents Types of AI Agents When to Use Agents? When Not to Use Agents? 10 Questions to Ask Before You Consider an AI Agent 3 Interesting Real-World Use Cases of AI Agents10 30 21 31 31 32 33 34 34 35 35 35 37 4022 23 25Chapter 1: What are AI agentsChapter 2: Frameworks for Building Agents LangGraph vs. AutoGen vs. CrewAI Practical Considerations What Tools and Functionalities Do They Support? How Well Do They Maintain the Context? Are They Well-Organized and Easy to Interpret? What’s the Quality of Documentation? Do They Provide Multi-Agent Support? What About Caching? Looking at the Replay Functionality What About Code Execution? Human in the Loop Support? Popular Use Cases Centered Around These Frameworks7/27 28/43\n",
            "10 Mastering AI Agents Types of AI Agents Now that we’re familiar with what AI agents are, let’s look at different types of AI agents along with their characteristics, examples, and when you can use them. See Table 1.1 below to get a quick idea of the types of AI agents and where and when you can use them. Name of the agent Key Characteristics Examples Best For Fixed Automation: The Digital Assembly LineNo intelligence, predictable behavior, limited scopeRPA, email autoresponders, basic scriptsRepetitive tasks, structured data, no need for adaptability LLM-Enhanced: Smarter, but Not EinsteinContext-aware, rule- constrained, statelessEmail filters, content moderation, support ticket routingFlexible tasks, high- volume/low-stakes, cost- sensitive scenarios ReAct: Reasoning Meets ActionMulti-step workflows, dynamic planning, basic problem-solvingTravel planners, AI dungeon masters, project planning toolsStrategic planning, multi- stage queries, dynamic adjustments ReAct + RAG: Grounded IntelligenceExternal knowledge access, low hallucinations, real-time dataLegal research tools, medical assistants, technical supportHigh-stakes decisions, domain-specific tasks, real-time knowledge needs Tool-Enhanced: The Multi-TaskersMulti-tool integration, dynamic execution, high automationCode generation tools, data analysis botsComplex workflows requiring multiple tools and APIs Self-Reflecting: The PhilosophersMeta-cognition, explainability, self- improvementSelf-evaluating systems, QA agentsTasks requiring accountability and improvement Memory-Enhanced: The Personalized PowerhousesLong-term memory, personalization, adaptive learningProject management AI, personalized assistantsIndividualized experiences, long-term interactions Environment Controllers: The World ShapersActive environment control, autonomous operation, feedback-drivenAutoGPT, adaptive robotics, smart citiesSystem control, IoT integration, autonomous operations Self-Learning: The EvolutionariesAutonomous learning, adaptive/scalable, evolutionary behaviorNeural networks, swarm AI, financial prediction modelsCutting-edge research, autonomous learning systems Table 1.1: Types of agents and their characteristics\n",
            "WHAT ARE AI \n",
            "AGENTS?01 \n",
            "CHAPTER\n",
            "\n",
            "   Query: What are the different types of agent\n",
            "\n",
            "   Answer:                      \n",
            "What are the types of agent\n",
            "A-A-B-B-C-D-E-F-E-G-I-L-M-P-C-D-E-F-E-G-I-L-M-P-C-D-E-F-E-G-I-L-M-P-C-D-E-F-E-G-I-L-M-P-C-D-E-F-E-G-I-L-M-P-C-D-E-F-E-G-I-L-M-P-C-D-E-F-E-G-I-L-M-P-C-D-E-F-E-G-I-L-M-P-C-D\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f6feaf3e-a2be-4c74-bad5-0c37bbe110b5",
      "metadata": {
        "id": "f6feaf3e-a2be-4c74-bad5-0c37bbe110b5"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n",
        "import concurrent\n",
        "import PyPDF2\n",
        "import os\n",
        "import pandas as pd\n",
        "import base64\n",
        "\n",
        "client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))\n",
        "dir_pdfs = 'openai_blog_pdfs' # have those PDFs stored locally here\n",
        "pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43e5cb9c-fc99-45e2-bd79-9c9ba5b410cc",
      "metadata": {
        "id": "43e5cb9c-fc99-45e2-bd79-9c9ba5b410cc"
      },
      "source": [
        "### Creating Vector Store with our PDFs\n",
        "\n",
        "We will create a Vector Store on OpenAI API and upload our PDFs to the Vector Store. OpenAI will read those PDFs, separate the content into multiple chunks of text, run embeddings on those and store those embeddings and the text in the Vector Store. It will enable us to query this Vector Store to return relevant content based on a query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "a6823030-9110-4143-ab7c-a223182eb7e0",
      "metadata": {
        "id": "a6823030-9110-4143-ab7c-a223182eb7e0"
      },
      "outputs": [],
      "source": [
        "def upload_single_pdf(file_path: str, vector_store_id: str):\n",
        "    file_name = os.path.basename(file_path)\n",
        "    try:\n",
        "        file_response = client.files.create(file=open(file_path, 'rb'), purpose=\"assistants\")\n",
        "        attach_response = client.vector_stores.files.create(\n",
        "            vector_store_id=vector_store_id,\n",
        "            file_id=file_response.id\n",
        "        )\n",
        "        return {\"file\": file_name, \"status\": \"success\"}\n",
        "    except Exception as e:\n",
        "        print(f\"Error with {file_name}: {str(e)}\")\n",
        "        return {\"file\": file_name, \"status\": \"failed\", \"error\": str(e)}\n",
        "\n",
        "def upload_pdf_files_to_vector_store(vector_store_id: str):\n",
        "    pdf_files = [os.path.join(dir_pdfs, f) for f in os.listdir(dir_pdfs)]\n",
        "    stats = {\"total_files\": len(pdf_files), \"successful_uploads\": 0, \"failed_uploads\": 0, \"errors\": []}\n",
        "\n",
        "    print(f\"{len(pdf_files)} PDF files to process. Uploading in parallel...\")\n",
        "\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
        "        futures = {executor.submit(upload_single_pdf, file_path, vector_store_id): file_path for file_path in pdf_files}\n",
        "        for future in tqdm(concurrent.futures.as_completed(futures), total=len(pdf_files)):\n",
        "            result = future.result()\n",
        "            if result[\"status\"] == \"success\":\n",
        "                stats[\"successful_uploads\"] += 1\n",
        "            else:\n",
        "                stats[\"failed_uploads\"] += 1\n",
        "                stats[\"errors\"].append(result)\n",
        "\n",
        "    return stats\n",
        "\n",
        "def create_vector_store(store_name: str) -> dict:\n",
        "    try:\n",
        "        vector_store = client.vector_stores.create(name=store_name)\n",
        "        details = {\n",
        "            \"id\": vector_store.id,\n",
        "            \"name\": vector_store.name,\n",
        "            \"created_at\": vector_store.created_at,\n",
        "            \"file_count\": vector_store.file_counts.completed\n",
        "        }\n",
        "        print(\"Vector store created:\", details)\n",
        "        return details\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating vector store: {e}\")\n",
        "        return {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5cb6cba0-931e-426a-88aa-34a62cc7158c",
      "metadata": {
        "id": "5cb6cba0-931e-426a-88aa-34a62cc7158c",
        "outputId": "fffde4e2-8db7-4a6e-8403-eb7fe8dcdcaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector store created: {'id': 'vs_6857dd3e1dec8191b1de5c533c4748cb', 'name': 'ai_agents_store', 'created_at': 1750588734, 'file_count': 0}\n",
            "2 PDF files to process. Uploading in parallel...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:08<00:00,  4.23s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'total_files': 2, 'successful_uploads': 2, 'failed_uploads': 0, 'errors': []}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "store_name = \"ai_agents_store\"\n",
        "vector_store_details = create_vector_store(store_name)\n",
        "upload_pdf_files_to_vector_store(vector_store_details[\"id\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f4ade3-2b3e-4df6-a441-c1ee3ea73172",
      "metadata": {
        "id": "e5f4ade3-2b3e-4df6-a441-c1ee3ea73172"
      },
      "source": [
        "### Standalone vector search\n",
        "\n",
        "Now that our vector store is ready, we are able to query the Vector Store directly and retrieve relevant content for a specific query. Using the new [vector search API](https://platform.openai.com/docs/api-reference/vector-stores/search), we're able to find relevant items from our knowledge base without necessarily integrating it in an LLM query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "980323d0-0112-4c9e-9b90-67719739026f",
      "metadata": {
        "id": "980323d0-0112-4c9e-9b90-67719739026f"
      },
      "outputs": [],
      "source": [
        "query = \"What's AI agents\"\n",
        "search_results = client.vector_stores.search(\n",
        "    vector_store_id=vector_store_details['id'],\n",
        "    query=query\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c6045a2e-a75f-48c0-89f4-841ef722d24f",
      "metadata": {
        "id": "c6045a2e-a75f-48c0-89f4-841ef722d24f",
        "outputId": "aaa39d1a-7411-410e-ef9e-3f42aa3bab94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3582 of character of content from Mastering AI agents.pdf with a relevant score of 0.9482921985607468\n",
            "3515 of character of content from Mastering AI agents.pdf with a relevant score of 0.9348534074942466\n",
            "3145 of character of content from Newwhitepaper_Agents.pdf with a relevant score of 0.886270160325278\n",
            "3856 of character of content from Mastering AI agents.pdf with a relevant score of 0.8826236858030915\n",
            "3373 of character of content from Mastering AI agents.pdf with a relevant score of 0.8796075510298714\n",
            "3084 of character of content from Mastering AI agents.pdf with a relevant score of 0.8795344108551211\n",
            "3847 of character of content from Mastering AI agents.pdf with a relevant score of 0.87755298161713\n",
            "3681 of character of content from Mastering AI agents.pdf with a relevant score of 0.8752338024110329\n",
            "3897 of character of content from Mastering AI agents.pdf with a relevant score of 0.8689723378176969\n",
            "3700 of character of content from Mastering AI agents.pdf with a relevant score of 0.845186560230217\n"
          ]
        }
      ],
      "source": [
        "for result in search_results.data:\n",
        "    print(str(len(result.content[0].text)) + ' of character of content from ' + result.filename + ' with a relevant score of ' + str(result.score))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4b0b4ec-ea13-429a-a1b7-7bac3d2ea014",
      "metadata": {
        "id": "d4b0b4ec-ea13-429a-a1b7-7bac3d2ea014"
      },
      "source": [
        "We can see that different size (and under-the-hood different texts) have been returned from the search query. They all have different relevancy score that are calculated by our ranker which uses hybrid search.\n",
        "\n",
        "### Integrating search results with LLM in a single API call\n",
        "\n",
        "However instead of querying the vector store and then passing the data into the Responses or Chat Completion API call, an even more convenient way to use this search results in an LLM query would be to plug use file_search tool as part of OpenAI Responses API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a153cb6e-e94b-4b55-a557-4f34fd3022bd",
      "metadata": {
        "id": "a153cb6e-e94b-4b55-a557-4f34fd3022bd",
        "outputId": "92d24147-c483-42ea-b860-d09a648fc566",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-21-3369251546.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"What's AI agents\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m response = client.responses.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     tools=[{\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/responses/responses.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, input, model, background, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, service_tier, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNOT_GIVEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[0;32m--> 690\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    691\u001b[0m             \u001b[0;34m\"/responses\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1037\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ],
      "source": [
        "query = \"What's AI agents\"\n",
        "response = client.responses.create(\n",
        "    input= query,\n",
        "    model=\"gpt-4o-mini\",\n",
        "    tools=[{\n",
        "        \"type\": \"file_search\",\n",
        "        \"vector_store_ids\": [vector_store_details['id']],\n",
        "    }]\n",
        ")\n",
        "\n",
        "# Extract annotations from the response\n",
        "annotations = response.output[1].content[0].annotations\n",
        "\n",
        "# Get top-k retrieved filenames\n",
        "retrieved_files = set([result.filename for result in annotations])\n",
        "\n",
        "print(f'Files used: {retrieved_files}')\n",
        "print('Response:')\n",
        "print(response.output[1].content[0].text) # 0 being the filesearch call"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c7b7b3-7d63-4630-95e7-76cf8080477e",
      "metadata": {
        "id": "e6c7b7b3-7d63-4630-95e7-76cf8080477e"
      },
      "source": [
        "We can see that `gpt-4o-mini` was able to answer a query that required more recent, specialised knowledge about OpenAI's Deep Research. It used content from the file `Introducing deep research _ OpenAI.pdf` that had chunks of texts that were the most relevant. If we want to go even deeper in the analysis of chunk of text retrieved, we can also analyse the different texts that were returned by the search engine by adding `include=[\"output[*].file_search_call.search_results\"]` to our query.\n",
        "\n",
        "## Evaluating performance\n",
        "\n",
        "What is key for those information retrieval system is to also measure the relevance & quality of files retrieved for those answers. The following steps of this cookbook will consist in generating an evaluation dataset and calculating different metrics over this generated dataset. This is an imperfect approach and we'll always recommend to have a human-verified evaluation dataset for your own use-cases, but it will show you the methodology to evaluate those.  It will be imperfect because some of the questions generated might be generic (e.g: What's said by the main stakeholder in this document) and our retrieval test will have a hard time to figure out which document that question was generated for."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93291578-d04a-4e71-8ecb-9f0f647e68c3",
      "metadata": {
        "id": "93291578-d04a-4e71-8ecb-9f0f647e68c3"
      },
      "source": [
        "### Generating questions\n",
        "\n",
        "We will create functions that will read through the PDFs we have locally and generate a question that can only be answered by this document. Therefore it'll create our evaluation dataset that we can use after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a1274ce-a468-489a-9206-0ff6ba82e8e7",
      "metadata": {
        "id": "2a1274ce-a468-489a-9206-0ff6ba82e8e7"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, \"rb\") as f:\n",
        "            reader = PyPDF2.PdfReader(f)\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text += page_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def generate_questions(pdf_path):\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    prompt = (\n",
        "        \"Can you generate a question that can only be answered from this document?:\\n\"\n",
        "        f\"{text}\\n\\n\"\n",
        "    )\n",
        "\n",
        "    response = client.responses.create(\n",
        "        input=prompt,\n",
        "        model=\"gpt-4o\",\n",
        "    )\n",
        "\n",
        "    question = response.output[0].content[0].text\n",
        "\n",
        "    return question"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7850d17f-832f-4a03-8216-5200d2db6b17",
      "metadata": {
        "id": "7850d17f-832f-4a03-8216-5200d2db6b17"
      },
      "source": [
        "If we run the function generate_question for the first PDF file we will be able to see the kind of question it generates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d19e4f5-a193-4787-aad1-8547173d36f4",
      "metadata": {
        "id": "4d19e4f5-a193-4787-aad1-8547173d36f4",
        "outputId": "b0db59f0-1bbc-4d23-cdef-55e6756a8df1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'What new capabilities will ChatGPT have as a result of the partnership between OpenAI and Schibsted Media Group?'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_questions(pdf_files[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc2e4e26-3396-4a3b-83a9-db9ae1597e41",
      "metadata": {
        "id": "dc2e4e26-3396-4a3b-83a9-db9ae1597e41"
      },
      "source": [
        "We can now generate all the questions for all the PDFs we've got stored locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fec6e6c-13b6-4498-b49c-d20e28b39ce9",
      "metadata": {
        "id": "0fec6e6c-13b6-4498-b49c-d20e28b39ce9"
      },
      "outputs": [],
      "source": [
        "# Generate questions for each PDF and store in a dictionary\n",
        "questions_dict = {}\n",
        "for pdf_path in pdf_files:\n",
        "    questions = generate_questions(pdf_path)\n",
        "    questions_dict[os.path.basename(pdf_path)] = questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e04371b-32ef-48f9-833a-84f53b7399fa",
      "metadata": {
        "id": "2e04371b-32ef-48f9-833a-84f53b7399fa",
        "outputId": "9520ff20-7fcd-4be0-b499-4cca0da93182"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'OpenAI partners with Schibsted Media Group _ OpenAI.pdf': 'What is the purpose of the partnership between Schibsted Media Group and OpenAI announced on February 10, 2025?',\n",
              " 'OpenAI and the CSU system bring AI to 500,000 students & faculty _ OpenAI.pdf': 'What significant milestone did the California State University system achieve by partnering with OpenAI, making it the first of its kind in the United States?',\n",
              " '1,000 Scientist AI Jam Session _ OpenAI.pdf': 'What was the specific AI model used during the \"1,000 Scientist AI Jam Session\" event across the nine national labs?',\n",
              " 'Announcing The Stargate Project _ OpenAI.pdf': 'What are the initial equity funders and lead partners in The Stargate Project announced by OpenAI, and who holds the financial and operational responsibilities?',\n",
              " 'Introducing Operator _ OpenAI.pdf': 'What is the name of the new model that powers the Operator agent introduced by OpenAI?',\n",
              " 'Introducing NextGenAI _ OpenAI.pdf': 'What major initiative did OpenAI launch on March 4, 2025, and which research institution from Europe is involved as a founding partner?',\n",
              " 'Introducing the Intelligence Age _ OpenAI.pdf': \"What is the name of the video generation tool used by OpenAI's creative team to help produce their Super Bowl ad?\",\n",
              " 'Operator System Card _ OpenAI.pdf': 'What is the preparedness score for the \"Cybersecurity\" category according to the Operator System Card?',\n",
              " 'Strengthening America’s AI leadership with the U.S. National Laboratories _ OpenAI.pdf': \"What is the purpose of OpenAI's agreement with the U.S. National Laboratories as described in the document?\",\n",
              " 'OpenAI GPT-4.5 System Card _ OpenAI.pdf': 'What is the Preparedness Framework rating for \"Cybersecurity\" for GPT-4.5 according to the system card?',\n",
              " 'Partnering with Axios expands OpenAI’s work with the news industry _ OpenAI.pdf': \"What is the goal of OpenAI's new content partnership with Axios as announced in the document?\",\n",
              " 'OpenAI and Guardian Media Group launch content partnership _ OpenAI.pdf': 'What is the main purpose of the partnership between OpenAI and Guardian Media Group announced on February 14, 2025?',\n",
              " 'Introducing GPT-4.5 _ OpenAI.pdf': 'What is the release date of the GPT-4.5 research preview?',\n",
              " 'Introducing data residency in Europe _ OpenAI.pdf': 'What are the benefits of data residency in Europe for new ChatGPT Enterprise and Edu customers according to the document?',\n",
              " 'The power of personalized AI _ OpenAI.pdf': 'What is the purpose of the \"Model Spec\" document published by OpenAI for ChatGPT?',\n",
              " 'Disrupting malicious uses of AI _ OpenAI.pdf': \"What is OpenAI's mission as stated in the document?\",\n",
              " 'Sharing the latest Model Spec _ OpenAI.pdf': 'What is the release date of the latest Model Spec mentioned in the document?',\n",
              " 'Deep research System Card _ OpenAI.pdf': \"What specific publication date is mentioned in the Deep Research System Card for when the report on deep research's preparedness was released?\",\n",
              " 'Bertelsmann powers creativity and productivity with OpenAI _ OpenAI.pdf': 'What specific AI-powered solutions is Bertelsmann planning to implement for its divisions RTL Deutschland and Penguin Random House according to the document?',\n",
              " 'OpenAI’s Economic Blueprint _ OpenAI.pdf': 'What date and location is scheduled for the kickoff event of OpenAI\\'s \"Innovating for America\" initiative as mentioned in the Economic Blueprint document?',\n",
              " 'Introducing deep research _ OpenAI.pdf': 'What specific model powers the \"deep research\" capability in ChatGPT that is discussed in this document, and what are its main features designed for?'}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "questions_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eea9bd1b-f746-4442-9f1b-aa31b5c766c6",
      "metadata": {
        "id": "eea9bd1b-f746-4442-9f1b-aa31b5c766c6"
      },
      "source": [
        "We now have a dictionary of `filename:question` that we can loop through and ask gpt-4o(-mini) about without providing the document, and gpt-4o should be able to find the relevant document in the Vector Store."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbda554b-c3d4-4b07-9028-b41670c2fa20",
      "metadata": {
        "id": "dbda554b-c3d4-4b07-9028-b41670c2fa20"
      },
      "source": [
        "We'll convert our dictionary into a dataframe and process it using gpt-4o-mini. We will look out for the expected file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968d54af-55c0-4b21-9ed8-c57811f9700f",
      "metadata": {
        "id": "968d54af-55c0-4b21-9ed8-c57811f9700f"
      },
      "outputs": [],
      "source": [
        "rows = []\n",
        "for filename, query in questions_dict.items():\n",
        "    rows.append({\"query\": query, \"_id\": filename.replace(\".pdf\", \"\")})\n",
        "\n",
        "# Metrics evaluation parameters\n",
        "k = 5\n",
        "total_queries = len(rows)\n",
        "correct_retrievals_at_k = 0\n",
        "reciprocal_ranks = []\n",
        "average_precisions = []\n",
        "\n",
        "def process_query(row):\n",
        "    query = row['query']\n",
        "    expected_filename = row['_id'] + '.pdf'\n",
        "    # Call file_search via Responses API\n",
        "    response = client.responses.create(\n",
        "        input=query,\n",
        "        model=\"gpt-4o-mini\",\n",
        "        tools=[{\n",
        "            \"type\": \"file_search\",\n",
        "            \"vector_store_ids\": [vector_store_details['id']],\n",
        "            \"max_num_results\": k,\n",
        "        }],\n",
        "        tool_choice=\"required\" # it will force the file_search, while not necessary, it's better to enforce it as this is what we're testing\n",
        "    )\n",
        "    # Extract annotations from the response\n",
        "    annotations = None\n",
        "    if hasattr(response.output[1], 'content') and response.output[1].content:\n",
        "        annotations = response.output[1].content[0].annotations\n",
        "    elif hasattr(response.output[1], 'annotations'):\n",
        "        annotations = response.output[1].annotations\n",
        "\n",
        "    if annotations is None:\n",
        "        print(f\"No annotations for query: {query}\")\n",
        "        return False, 0, 0\n",
        "\n",
        "    # Get top-k retrieved filenames\n",
        "    retrieved_files = [result.filename for result in annotations[:k]]\n",
        "    if expected_filename in retrieved_files:\n",
        "        rank = retrieved_files.index(expected_filename) + 1\n",
        "        rr = 1 / rank\n",
        "        correct = True\n",
        "    else:\n",
        "        rr = 0\n",
        "        correct = False\n",
        "\n",
        "    # Calculate Average Precision\n",
        "    precisions = []\n",
        "    num_relevant = 0\n",
        "    for i, fname in enumerate(retrieved_files):\n",
        "        if fname == expected_filename:\n",
        "            num_relevant += 1\n",
        "            precisions.append(num_relevant / (i + 1))\n",
        "    avg_precision = sum(precisions) / len(precisions) if precisions else 0\n",
        "\n",
        "    if expected_filename not in retrieved_files:\n",
        "        print(\"Expected file NOT found in the retrieved files!\")\n",
        "\n",
        "    if retrieved_files and retrieved_files[0] != expected_filename:\n",
        "        print(f\"Query: {query}\")\n",
        "        print(f\"Expected file: {expected_filename}\")\n",
        "        print(f\"First retrieved file: {retrieved_files[0]}\")\n",
        "        print(f\"Retrieved files: {retrieved_files}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "\n",
        "    return correct, rr, avg_precision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee6d3084-5fae-4a26-8fd2-d269ffbc60ee",
      "metadata": {
        "id": "ee6d3084-5fae-4a26-8fd2-d269ffbc60ee",
        "outputId": "38743141-06a6-403c-f4b6-92b4ada382bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(True, 1.0, 1.0)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_query(rows[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba088faf-2945-48b3-a3de-412da1ee81fc",
      "metadata": {
        "id": "ba088faf-2945-48b3-a3de-412da1ee81fc"
      },
      "source": [
        "Recall & Precision are at 1 for this example, and our file ranked first so we're having a MRR and MAP = 1 on this example.\n",
        "\n",
        "We can now execute this processing on our set of questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f1e1cc2-0128-48cc-9e4c-5eb416c21347",
      "metadata": {
        "id": "6f1e1cc2-0128-48cc-9e4c-5eb416c21347",
        "outputId": "d134bd29-4562-4b3e-953a-c3de4e4fa90d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|███████████████████▏           | 13/21 [00:07<00:03,  2.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected file NOT found in the retrieved files!\n",
            "Query: What is OpenAI's mission as stated in the document?\n",
            "Expected file: Disrupting malicious uses of AI _ OpenAI.pdf\n",
            "First retrieved file: Introducing the Intelligence Age _ OpenAI.pdf\n",
            "Retrieved files: ['Introducing the Intelligence Age _ OpenAI.pdf']\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 71%|██████████████████████▏        | 15/21 [00:14<00:06,  1.04s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected file NOT found in the retrieved files!\n",
            "Query: What is the purpose of the \"Model Spec\" document published by OpenAI for ChatGPT?\n",
            "Expected file: The power of personalized AI _ OpenAI.pdf\n",
            "First retrieved file: Sharing the latest Model Spec _ OpenAI.pdf\n",
            "Retrieved files: ['Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf', 'Sharing the latest Model Spec _ OpenAI.pdf']\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|███████████████████████████████| 21/21 [00:15<00:00,  1.38it/s]\n"
          ]
        }
      ],
      "source": [
        "with ThreadPoolExecutor() as executor:\n",
        "    results = list(tqdm(executor.map(process_query, rows), total=total_queries))\n",
        "\n",
        "correct_retrievals_at_k = 0\n",
        "reciprocal_ranks = []\n",
        "average_precisions = []\n",
        "\n",
        "for correct, rr, avg_precision in results:\n",
        "    if correct:\n",
        "        correct_retrievals_at_k += 1\n",
        "    reciprocal_ranks.append(rr)\n",
        "    average_precisions.append(avg_precision)\n",
        "\n",
        "recall_at_k = correct_retrievals_at_k / total_queries\n",
        "precision_at_k = recall_at_k  # In this context, same as recall\n",
        "mrr = sum(reciprocal_ranks) / total_queries\n",
        "map_score = sum(average_precisions) / total_queries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bc74d02-7ee9-4cc3-b48f-5c205c3fdfcb",
      "metadata": {
        "id": "6bc74d02-7ee9-4cc3-b48f-5c205c3fdfcb"
      },
      "source": [
        "The outputs logged above would either show that a file wasn't ranked first when our evaluation dataset expected it to rank first or that it wasn't found at all. As we can see from our imperfect evaluation dataset, some questions were generic and expected another doc, which our retrieval system didn't specifically retrieved for this question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a32ec63-8f39-4085-b123-f2593eb702d3",
      "metadata": {
        "id": "7a32ec63-8f39-4085-b123-f2593eb702d3",
        "outputId": "e4f6c102-6e0d-46d0-920c-2254ff60ed2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics at k=5:\n",
            "Recall@5: 0.9048\n",
            "Precision@5: 0.9048\n",
            "Mean Reciprocal Rank (MRR): 0.9048\n",
            "Mean Average Precision (MAP): 0.8954\n"
          ]
        }
      ],
      "source": [
        "# Print the metrics with k\n",
        "print(f\"Metrics at k={k}:\")\n",
        "print(f\"Recall@{k}: {recall_at_k:.4f}\")\n",
        "print(f\"Precision@{k}: {precision_at_k:.4f}\")\n",
        "print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
        "print(f\"Mean Average Precision (MAP): {map_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34d19556-8d99-4c53-8800-eec54948a674",
      "metadata": {
        "id": "34d19556-8d99-4c53-8800-eec54948a674"
      },
      "source": [
        "With this cookbook we were able to see how to:\n",
        "- Generate a dataset of evaluations using PDF context-stuffing (leveraging vision modality of 4o) and traditional PDF readers\n",
        "- Create a vector store and populate it with PDF\n",
        "- Get an LLM answer to a query, leveraging a RAG system available out-of-the-box with `file_search` tool call in OpenAI's Response API\n",
        "- Understand how chunks of texts are retrieved, ranked and used as part of the Response API\n",
        "- Measure accuracy, precision, retrieval, MRR and MAP on the dataset of evaluations previously generated\n",
        "\n",
        "By using file search with Responses, you can simplify RAG architecture and leverage this in a single API call using the new Responses API. File storage, embeddings, retrieval all integrated in one tool!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6eee51142df3499aa7705a686a5b7b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8907ae51048142fcbe8fb9cc86494ccd",
              "IPY_MODEL_61f365f727164f0e805574e60bb5082e",
              "IPY_MODEL_1c3d5ecb909d4d58b74ea65573e8c035"
            ],
            "layout": "IPY_MODEL_7d459cfe0ebd48bca6f4f29a6f4ae7d9"
          }
        },
        "8907ae51048142fcbe8fb9cc86494ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e5f384dcd0d487a89db977f006e8c93",
            "placeholder": "​",
            "style": "IPY_MODEL_2edc3327b9164abf8a5478a70e6138eb",
            "value": "Batches: 100%"
          }
        },
        "61f365f727164f0e805574e60bb5082e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfa5ad9c84834fb2ba67781d99e97ce0",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2c5d568ca544dc6855b00e52524d56a",
            "value": 3
          }
        },
        "1c3d5ecb909d4d58b74ea65573e8c035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6f056ba4c0b4e3ebb641d33e61fdabf",
            "placeholder": "​",
            "style": "IPY_MODEL_697b12d398ef45fdb44ff9cc532b36bd",
            "value": " 3/3 [00:13&lt;00:00,  4.18s/it]"
          }
        },
        "7d459cfe0ebd48bca6f4f29a6f4ae7d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e5f384dcd0d487a89db977f006e8c93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2edc3327b9164abf8a5478a70e6138eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfa5ad9c84834fb2ba67781d99e97ce0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2c5d568ca544dc6855b00e52524d56a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6f056ba4c0b4e3ebb641d33e61fdabf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "697b12d398ef45fdb44ff9cc532b36bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}